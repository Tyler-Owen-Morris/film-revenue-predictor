{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss, mean_squared_error, r2_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import  RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from math import sqrt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data and clean it up with all the functions from our EDA\n",
    "\n",
    "We will be using the string hashing trick to attempt a better fit than our initial methods. For this reason we're going to conform as many of our columns to individual strings as possible, and then turn the row into 1 string for processing with the FeatureHasher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2965, 26)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2019 = pd.read_csv('../data/IMDB_mine_data_2019.csv',index_col=0)\n",
    "_2018 = pd.read_csv('../data/IMDB_mine_data_2018.csv',index_col=0)\n",
    "_2017 = pd.read_csv('../data/IMDB_mine_data_2017.csv',index_col=0)\n",
    "_2016 = pd.read_csv('../data/IMDB_mine_data_2016.csv',index_col=0)\n",
    "_2015 = pd.read_csv('../data/IMDB_mine_data_2015.csv',index_col=0)\n",
    "#get all the films into one DF\n",
    "films = pd.concat([_2019,_2018,_2017,_2016,_2015])\n",
    "# remove the filler films we were using to start the mining bot\n",
    "films = films[films['title_code'] != np.nan]\n",
    "films = films[films['opening_wknd'] != np.nan]\n",
    "films = films[films['release_date'] != '1980-05-16']\n",
    "films.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>runtime</th>\n",
       "      <th>release_date</th>\n",
       "      <th>rating</th>\n",
       "      <th>prod_co</th>\n",
       "      <th>metaScore</th>\n",
       "      <th>metaUserScore</th>\n",
       "      <th>imdb_rating</th>\n",
       "      <th>genre</th>\n",
       "      <th>actor1</th>\n",
       "      <th>actor2</th>\n",
       "      <th>actor3</th>\n",
       "      <th>actor4</th>\n",
       "      <th>actor5</th>\n",
       "      <th>actor6</th>\n",
       "      <th>actor7</th>\n",
       "      <th>actor8</th>\n",
       "      <th>actor9</th>\n",
       "      <th>actor10</th>\n",
       "      <th>directors</th>\n",
       "      <th>writers</th>\n",
       "      <th>budget</th>\n",
       "      <th>opening_wknd</th>\n",
       "      <th>gross_dom</th>\n",
       "      <th>gross_int</th>\n",
       "      <th>title_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Motherless Brooklyn</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>R</td>\n",
       "      <td>['Class 5 Films', 'Warner Bros.']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Crime', 'Drama', 'Mystery']</td>\n",
       "      <td>/name/nm0001570/</td>\n",
       "      <td>/name/nm1813221/</td>\n",
       "      <td>/name/nm0000285/</td>\n",
       "      <td>/name/nm0134072/</td>\n",
       "      <td>/name/nm0000353/</td>\n",
       "      <td>/name/nm0000246/</td>\n",
       "      <td>/name/nm0839486/</td>\n",
       "      <td>/name/nm0427728/</td>\n",
       "      <td>/name/nm1316767/</td>\n",
       "      <td>/name/nm0656929/</td>\n",
       "      <td>['Edward Norton']</td>\n",
       "      <td>['Jonathan Lethem', 'Edward Norton']</td>\n",
       "      <td>26000000.0</td>\n",
       "      <td>3500454.0</td>\n",
       "      <td>9277736.0</td>\n",
       "      <td>18477736.0</td>\n",
       "      <td>tt0385887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alita: Battle Angel</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-02-14</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>['Twentieth Century Fox', 'Twentieth Century F...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Action', 'Adventure', 'Sci-Fi', 'Thriller']</td>\n",
       "      <td>/name/nm4023073/</td>\n",
       "      <td>/name/nm0910607/</td>\n",
       "      <td>/name/nm0000124/</td>\n",
       "      <td>/name/nm0991810/</td>\n",
       "      <td>/name/nm4534098/</td>\n",
       "      <td>/name/nm0355097/</td>\n",
       "      <td>/name/nm5277107/</td>\n",
       "      <td>/name/nm7449863/</td>\n",
       "      <td>/name/nm7093076/</td>\n",
       "      <td>/name/nm1701107/</td>\n",
       "      <td>['Robert Rodriguez']</td>\n",
       "      <td>['James Cameron', 'Laeta Kalogridis', 'Yukito ...</td>\n",
       "      <td>170000000.0</td>\n",
       "      <td>28525613.0</td>\n",
       "      <td>85710210.0</td>\n",
       "      <td>404852543.0</td>\n",
       "      <td>tt0437086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danger Close</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-11-08</td>\n",
       "      <td>R</td>\n",
       "      <td>['Deeper Water', 'Saboteur Media', 'Saban Films']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['Action', 'Drama', 'War']</td>\n",
       "      <td>/name/nm1379938/</td>\n",
       "      <td>/name/nm9826817/</td>\n",
       "      <td>/name/nm1542397/</td>\n",
       "      <td>/name/nm2527406/</td>\n",
       "      <td>/name/nm5937328/</td>\n",
       "      <td>/name/nm9680111/</td>\n",
       "      <td>/name/nm3478396/</td>\n",
       "      <td>/name/nm7011217/</td>\n",
       "      <td>/name/nm2828232/</td>\n",
       "      <td>/name/nm7202582/</td>\n",
       "      <td>['Kriv Stenders']</td>\n",
       "      <td>['Stuart Beattie', 'James Nicholas', 'Karel Se...</td>\n",
       "      <td>35000000.0</td>\n",
       "      <td>2078370.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tt0441881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title  runtime release_date rating  \\\n",
       "0  Motherless Brooklyn        0   2019-11-01      R   \n",
       "0  Alita: Battle Angel        0   2019-02-14  PG-13   \n",
       "0         Danger Close        0   2019-11-08      R   \n",
       "\n",
       "                                             prod_co  metaScore  \\\n",
       "0                  ['Class 5 Films', 'Warner Bros.']          0   \n",
       "0  ['Twentieth Century Fox', 'Twentieth Century F...          0   \n",
       "0  ['Deeper Water', 'Saboteur Media', 'Saban Films']          0   \n",
       "\n",
       "   metaUserScore  imdb_rating                                          genre  \\\n",
       "0              0            0                  ['Crime', 'Drama', 'Mystery']   \n",
       "0              0            0  ['Action', 'Adventure', 'Sci-Fi', 'Thriller']   \n",
       "0              0            0                     ['Action', 'Drama', 'War']   \n",
       "\n",
       "             actor1            actor2            actor3            actor4  \\\n",
       "0  /name/nm0001570/  /name/nm1813221/  /name/nm0000285/  /name/nm0134072/   \n",
       "0  /name/nm4023073/  /name/nm0910607/  /name/nm0000124/  /name/nm0991810/   \n",
       "0  /name/nm1379938/  /name/nm9826817/  /name/nm1542397/  /name/nm2527406/   \n",
       "\n",
       "             actor5            actor6            actor7            actor8  \\\n",
       "0  /name/nm0000353/  /name/nm0000246/  /name/nm0839486/  /name/nm0427728/   \n",
       "0  /name/nm4534098/  /name/nm0355097/  /name/nm5277107/  /name/nm7449863/   \n",
       "0  /name/nm5937328/  /name/nm9680111/  /name/nm3478396/  /name/nm7011217/   \n",
       "\n",
       "             actor9           actor10             directors  \\\n",
       "0  /name/nm1316767/  /name/nm0656929/     ['Edward Norton']   \n",
       "0  /name/nm7093076/  /name/nm1701107/  ['Robert Rodriguez']   \n",
       "0  /name/nm2828232/  /name/nm7202582/     ['Kriv Stenders']   \n",
       "\n",
       "                                             writers       budget  \\\n",
       "0               ['Jonathan Lethem', 'Edward Norton']   26000000.0   \n",
       "0  ['James Cameron', 'Laeta Kalogridis', 'Yukito ...  170000000.0   \n",
       "0  ['Stuart Beattie', 'James Nicholas', 'Karel Se...   35000000.0   \n",
       "\n",
       "   opening_wknd   gross_dom    gross_int title_code  \n",
       "0     3500454.0   9277736.0   18477736.0  tt0385887  \n",
       "0    28525613.0  85710210.0  404852543.0  tt0437086  \n",
       "0     2078370.0         NaN          NaN  tt0441881  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reset the index now that all films are in 1 dataframe\n",
    "#films = films.reset_index(drop=True)\n",
    "films.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean some of the columns and map the features to a cleaner structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the text in the production company column, and turn it into an accessable array\n",
    "films['prod_co'] = films.prod_co.map(lambda x : re.findall(r\"'(.*?)'\",x, re.DOTALL))\n",
    "\n",
    "#break production and distribution out into their own columns\n",
    "films['production'] = films['prod_co'].map(lambda x : x[0] if len(x) >= 1 else np.nan)\n",
    "films['production_2'] = films['prod_co'].map(lambda x : x[1] if len(x) >= 3 else np.nan)\n",
    "films['distribution'] = films['prod_co'].map(lambda x : x[-1] if len(x) >= 2 else np.nan)\n",
    "\n",
    "#convert the release date to a pandas datetime object\n",
    "films['release_date'] = films['release_date'].map(lambda x : pd.to_datetime(x))\n",
    "\n",
    "#Set the first director to their own column\n",
    "films.directors = films.directors.map(lambda x : re.findall(r\"'(.*?)'\",x, re.DOTALL if isinstance(x, str) else np.nan))\n",
    "films['director'] = films['directors'].map(lambda x: x[0].replace(\" \", '') if len(x) >= 1 else 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Actor names from the key we scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the actor codes to strings\n",
    "actor_key = pd.read_csv('../data/actor_key.csv', index_col=0).reset_index()\n",
    "\n",
    "def get_actor_name(key):\n",
    "    #print(key)\n",
    "    if isinstance(key, float):\n",
    "        return key\n",
    "    row = actor_key.loc[actor_key['actor'] == key].index[0]\n",
    "    #print(type(actor_key.iloc[row]['name']), actor_key.iloc[row]['name'])\n",
    "    return actor_key.iloc[row]['name']\n",
    "\n",
    "def get_actor_key(name):\n",
    "    #print(key)\n",
    "    row = actor_key.loc[actor_key['name'] == name].index[0]\n",
    "    return(actor_key.iloc[row]['actor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "films['actor_1'] = films['actor1'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_2'] = films['actor2'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_3'] = films['actor3'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_4'] = films['actor4'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_5'] = films['actor5'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_6'] = films['actor6'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_7'] = films['actor7'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_8'] = films['actor8'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_9'] = films['actor9'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)\n",
    "films['actor_10'] = films['actor10'].map(lambda x : get_actor_name(x).replace(\" \", '') if isinstance(x, str) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the producers with their films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the main producer and executive producer to the dataframe\n",
    "producer_key = pd.read_csv('../data/producer_key.csv', index_col=0)\n",
    "films = films.merge(producer_key, on='title_code', how='left')\n",
    "#remove spaces in the names, and replace the fill values with empty strings\n",
    "films['producer'] = films['producer'].map(lambda x : x.replace(\" \", '') if x!='[]' else None)\n",
    "films['executive'] = films['executive'].map(lambda x : x.replace(\" \", '') if x!='[]' else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the release month as a string column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reencode the release month as a string.\n",
    "films['release_month'] = films['release_date'].map(lambda x : pd.to_datetime(x).month)\n",
    "films['release_month'] = films['release_month'].map(lambda x : 'January' if x == 1 else ( 'February' if x==2 else ( 'March' if x==3 else ( 'April' if x==4 else ('May' if x==5 else ('June' if x==6 else ( 'July' if x==7 else ( 'August' if x==8 else ('September' if x==9 else ( 'October' if x==10 else ( 'November' if x==11 else ('December' if x==12 else 'unknown'))))))))) ) ))\n",
    "#reencode the release year as a string\n",
    "films['release_year'] = films['release_date'].map(lambda x : x.year)\n",
    "films['release_year'] = films['release_year'].map(lambda x : '2015' if x == 2015 else ( '2016' if x==2016 else ( '2017' if x==2017 else ( '2018' if x==2018 else ('2019' if x==2019 else ('2020' if x==2020 else 'none'))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add all the strings together to simplify our string hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "films['train_string'] = films[['production','distribution','director','actor_1','actor_2','actor_3','actor_4',\n",
    "                               'actor_5','actor_6','actor_7','actor_8','actor_9','actor_10',\n",
    "                               'producer','executive']].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashing Features\n",
    "We are going to create a single matrix that we can train a model on that contains all of the strings in our films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2965x1100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 47774 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = HashingVectorizer(n_features=1000)\n",
    "vector = vectorizer.transform(films['train_string'].to_numpy())\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test split, and train on our string hashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model will guess: 6284064.0\n",
      "Our baseline RMSE is : 21204937\n"
     ]
    }
   ],
   "source": [
    "y = films['opening_wknd']\n",
    "X = vector\n",
    "\n",
    "dum_guess = round(films['opening_wknd'].mean())\n",
    "print(\"Base Model will guess: \"+ str(dum_guess))\n",
    "print(\"Our baseline RMSE is : \" + str(round(sqrt(mean_squared_error(y, np.full(y.shape,dum_guess))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 592690279751323.0\n",
      "RMSE: 24345231.150090218\n"
     ]
    }
   ],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(np.nan_to_num(X_train), y_train)\n",
    "lin_preds = lin_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, lin_preds)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, lin_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 357428987576626.4\n",
      "RMSE: 18905792.434506055\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=550,\n",
    "                           #max_features=3,\n",
    "                            max_depth=1000,\n",
    "                            min_samples_split=3,\n",
    "                            min_samples_leaf=5)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, rf_pred)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, rf_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 438710245648053.25\n",
      "RMSE: 20945411.088065404\n"
     ]
    }
   ],
   "source": [
    "model_g = GradientBoostingRegressor(learning_rate=0.025,\n",
    "                                   n_estimators=430,\n",
    "                                   min_samples_leaf=4,\n",
    "                                   max_depth=1100)\n",
    "model_g.fit(X_train,y_train)\n",
    "preds_gb = model_g.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, preds_gb)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, preds_gb))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 413871197730962.7\n",
      "RMSE: 20343824.56007136\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(n_estimators=100,\n",
    "                            max_depth=200).fit(X_train, y_train)\n",
    "xg_pred = xgb_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, xg_pred)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, xg_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great news!\n",
    "This means that we have some signal from our string vectors. Now we will add our one-hot features, and the other data points that we discovered in our inital EDA, and see if we can improve on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the genre OHE\n",
    "# we need to extract the inner quotes from the strings into a list.\n",
    "films['genre'] = films['genre'].map(lambda x : re.findall(r\"'(.*?)'\",x, re.DOTALL))\n",
    "#we're going to do the OHE manually\n",
    "films['action'] = films['genre'].map(lambda x : 1 if 'Action' in x else 0)\n",
    "films['adventure']  = films['genre'].map(lambda x : 1 if 'Adventure' in x else 0)\n",
    "films['animated'] = films['genre'].map(lambda x : 1 if 'Animation' in x else 0)\n",
    "films['biography'] = films['genre'].map(lambda x : 1 if 'Biography' in x else 0)\n",
    "films['drama'] = films['genre'].map(lambda x : 1 if 'Drama' in x else 0)\n",
    "films['documentary'] = films['genre'].map(lambda x : 1 if 'Documentary' in x else 0)\n",
    "films['comedy'] = films['genre'].map(lambda x : 1 if 'Comedy' in x else 0)\n",
    "films['crime'] = films['genre'].map(lambda x : 1 if 'Crime' in x else 0)\n",
    "films['fantasy'] = films['genre'].map(lambda x : 1 if 'Fantasy' in x else 0)\n",
    "films['family'] = films['genre'].map(lambda x : 1 if 'Family' in x else 0)\n",
    "films['musical'] = films['genre'].map(lambda x : 1 if 'Musical' in x else 0)\n",
    "films['horror'] = films['genre'].map(lambda x : 1 if 'Horror' in x else 0)\n",
    "films['war'] = films['genre'].map(lambda x : 1 if 'War' in x else 0)\n",
    "films['mystery'] = films['genre'].map(lambda x : 1 if 'Mystery' in x else 0)\n",
    "films['sci-fi'] = films['genre'].map(lambda x : 1 if 'Sci-Fi' in x else 0)\n",
    "films['thriller'] = films['genre'].map(lambda x : 1 if 'Thriller' in x else 0)\n",
    "films['romance'] = films['genre'].map(lambda x : 1 if 'Romance' in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../data/actor_popularity_out.csv does not exist: '../data/actor_popularity_out.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-161c1ae15253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfake_pop\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mact_pop_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/actor_popularity_out.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mact_pop_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File ../data/actor_popularity_out.csv does not exist: '../data/actor_popularity_out.csv'"
     ]
    }
   ],
   "source": [
    "#add actor popularity scores\n",
    "fake_popularity = 10071118 #instantiated as the lowest actor popularity +1\n",
    "def get_act_pop(code, fake_pop):\n",
    "    if code in act_pop_keys['actor'].unique():\n",
    "        row = act_pop_keys.loc[act_pop_keys['actor'] == code].index[0]\n",
    "        return act_pop_keys.iloc[row]['popularity']\n",
    "    else:\n",
    "        return fake_pop + randint(0,1000)\n",
    "\n",
    "act_pop_keys = pd.read_csv('../data/actor_popularity_out.csv', index_col=0)\n",
    "act_pop_keys.reset_index(inplace=True, drop=True)\n",
    "\n",
    "films['actor1_popularity'] = films['actor1'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor2_popularity'] = films['actor2'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor3_popularity'] = films['actor3'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor4_popularity'] = films['actor4'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor5_popularity'] = films['actor5'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor6_popularity'] = films['actor6'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor7_popularity'] = films['actor7'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor8_popularity'] = films['actor8'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor9_popularity'] = films['actor9'].map(lambda x : get_act_pop(x,fake_popularity))\n",
    "films['actor10_popularity'] = films['actor10'].map(lambda x : get_act_pop(x,fake_popularity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_celeb_class(pop):\n",
    "    if pop <=2000:\n",
    "        return 'A-list'\n",
    "    elif pop <= 5000:\n",
    "        return 'B-list'\n",
    "    elif pop <= 20000:\n",
    "        return 'C-list'\n",
    "    elif pop <= 100000:\n",
    "        return 'D-list'\n",
    "    elif pop <= 250000:\n",
    "        return 'E-list'\n",
    "    else:\n",
    "        return 'nobody'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert popularity scores to text columns \n",
    "films['actor1_class'] = films['actor1_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor2_class'] = films['actor2_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor3_class'] = films['actor3_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor4_class'] = films['actor4_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor5_class'] = films['actor5_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor6_class'] = films['actor6_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor7_class'] = films['actor7_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor8_class'] = films['actor8_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor9_class'] = films['actor9_popularity'].map(lambda x : get_celeb_class(x))\n",
    "films['actor10_class'] = films['actor10_popularity'].map(lambda x : get_celeb_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the normalized budget\n",
    "bud = films[['budget']].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "bud_scaled = min_max_scaler.fit_transform(bud)\n",
    "films['budget_normalized']=bud_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train new models with the full set of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will try to filter outliers this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*******this block filters by zscore*********\n",
    "# film_zscore = zscore(films.opening_wknd.to_numpy())\n",
    "# abs_film_zscore = np.abs(film_zscore)\n",
    "# filtered_entries = (abs_film_zscore < 3)\n",
    "# filtered_df = films[filtered_entries].reset_index(drop=True)\n",
    "\n",
    "#*******this filters by highest box office**********\n",
    "#filtered_df = films[films.opening_wknd < films.opening_wknd.quantile(.97)].reset_index(drop=True)\n",
    "\n",
    "#******this filters by largest and smallest box office to budget ratio**********\n",
    "films['pct_profit'] = films['opening_wknd'] / films['budget']\n",
    "take = .98\n",
    "filtered_df = films[films.['opening_wknd'] < films['opening_wknd'].quantile(.97)].reset_index(drop=True)\n",
    "filtered_df = filtered_df[filtered_df['pct_profit'] < filtered_df['pct_profit'].quantile(take)]\n",
    "filtered_df = filtered_df[filtered_df['pct_profit'] > filtered_df['pct_profit'].quantile(1-take)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#******this filters the films we suspect reported bad data**********\n",
    "# filtered_df = filtered_df.drop(filtered_df[(filtered_df['budget'] != filtered_df['opening_wknd']) & \n",
    "#                                            (filtered_df['budget'] < 150000)].index).reset_index(drop=True)\n",
    "\n",
    "filtered_df.info(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dummies from our curated columns\n",
    "dum = pd.get_dummies(filtered_df[['release_month', 'actor1_class', 'actor2_class', 'actor3_class','rating']]) #'actor1_class', 'actor2_class', 'actor3_class',\n",
    "dum.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull the columns we want from the main DF\n",
    "use_cols = filtered_df[['budget','action','adventure','animated','biography','drama','documentary','comedy','crime',\n",
    "                        'fantasy','family','musical','horror','war','mystery','sci-fi','thriller','romance']]\n",
    "                        #actor1_popularity', 'actor2_popularity', 'actor3_popularity', 'actor4_popularity']]\n",
    "use_cols.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(n_features=1000)\n",
    "vector = vectorizer.fit_transform(filtered_df['train_string'].to_numpy())\n",
    "vec_df = pd.DataFrame.sparse.from_spmatrix(vector)\n",
    "vec_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = filtered_df['opening_wknd']\n",
    "X = pd.concat([use_cols, dum, vec_df], axis=1, sort=False)\n",
    "X.shape, y.shape\n",
    "\n",
    "dum_guess = round(films['opening_wknd'].mean())\n",
    "print(\"Base Model will guess: \"+ str(dum_guess))\n",
    "print(\"Our baseline RMSE is : \" + str(round(sqrt(mean_squared_error(y, np.full(y.shape,dum_guess))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lin_model.fit(X_train, y_train)\n",
    "lin_preds = lin_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, lin_preds)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, lin_preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 34028724199220.074\n",
      "RMSE: 5833414.454607188\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=50,\n",
    "                           #max_features=3,\n",
    "                            max_depth=1000,\n",
    "                            min_samples_split=6,\n",
    "                            min_samples_leaf=8)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, rf_pred)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, rf_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 34529976540121.24\n",
      "RMSE: 5876221.280731456\n"
     ]
    }
   ],
   "source": [
    "model_g = GradientBoostingRegressor(learning_rate=0.02,\n",
    "                                   n_estimators=200,\n",
    "                                   min_samples_leaf=15,\n",
    "                                   max_depth=400)\n",
    "model_g.fit(X_train,y_train)\n",
    "preds_gb = model_g.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, preds_gb)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, preds_gb))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE : 42987803231494.375\n",
      "RMSE: 6556508.463465473\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor(n_estimators=230,\n",
    "                            max_depth=1000).fit(X_train, y_train)\n",
    "xg_pred = xgb_model.predict(X_test)\n",
    "print(\"MSE : \" + str(mean_squared_error(y_test, xg_pred)))\n",
    "print(\"RMSE: \" + str(sqrt(mean_squared_error(y_test, xg_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 30.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 73.5min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 125.1min\n"
     ]
    }
   ],
   "source": [
    "#gradient boost searching\n",
    "clf = GridSearchCV(model_g,\n",
    "                   {'max_depth': [200, 400, 600,1000,1500,2000],\n",
    "                    'n_estimators': [50, 100, 200,300,400,500],\n",
    "                   'min_samples_leaf':[4,8,10,15,18],\n",
    "                   'learning_rate':[0.005,0.01,0.02,0.3]}, verbose=2, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1470 candidates, totalling 7350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  9.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1136 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1552 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2032 tasks      | elapsed: 22.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 29.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 35.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3856 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4592 tasks      | elapsed: 50.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5392 tasks      | elapsed: 57.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6256 tasks      | elapsed: 66.1min\n",
      "[Parallel(n_jobs=-1)]: Done 7184 tasks      | elapsed: 75.5min\n",
      "[Parallel(n_jobs=-1)]: Done 7350 out of 7350 | elapsed: 76.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5278523053836944\n",
      "{'max_depth': 1000, 'min_samples_leaf': 8, 'min_samples_split': 6, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "clf = GridSearchCV(rf_model,\n",
    "                   {'max_depth': [200, 400, 600,1000,1500,2000,3000],\n",
    "                    'n_estimators': [50, 100, 200,300,400,500],\n",
    "                   'min_samples_leaf':[4,8,10,15,18],\n",
    "                   'min_samples_split':[3,6,10,12,15,18,20]}, verbose=3, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "importances = xgb_model.feature_importances_\n",
    "features = X.columns\n",
    "indicies = np.argsort(importances)\n",
    "#fig.title(\"Feature importance\")\n",
    "ax.barh(range(len(indicies)), importances[indicies])\n",
    "\n",
    "ax.set_title('Feature Importance', fontsize=26)\n",
    "plt.style.use('bmh')\n",
    "fig.patch.set_facecolor('black')\n",
    "#ax1.set_ylabel('Average Opening Revenue', fontsize=20)\n",
    "#ax1.set_xlabel('MPAA Rating', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
